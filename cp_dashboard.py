import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
# from nltk.tokenize import word_tokenize
import re
# from matplotlib.font_manager import FontProperties
from sklearn.feature_extraction.text import CountVectorizer
# from sklearn.decomposition import LatentDirichletAllocation
from transformers import pipeline
from dateutil.relativedelta import relativedelta
from datetime import datetime


nltk.download("stopwords")
nltk.download("wordnet")
nltk.download('punkt')

#########################################################

st.set_page_config(
    page_title="Sentiment Analaysis Dashboard", page_icon="ðŸ“Š", layout="wide"
)

adjust_top_pad = """
    <style>
        div.block-container {padding-top:1rem;}
    </style>
    """
st.markdown(adjust_top_pad, unsafe_allow_html=True)

st.title('Sentiment Analaysis Dashboard')

#########################################################

DATA_URL = (r"ikea_reviews_sentiment2.parquet")
data = pd.read_parquet(DATA_URL)
data['publishedAtDate'] = pd.to_datetime(data['publishedAtDate'])
data['date'] = data['publishedAtDate'].dt.date

# Function to load data and filter it based on language and date range
@st.cache_data
def load_and_filter_data(DATA_URL, language, store, start_date, end_date):
    data['date'] = data['publishedAtDate'].dt.date
    lowercase = lambda x: str(x).lower()
    data.rename(lowercase, axis='columns', inplace=True)
    
    # Group "Chinese_China," "Chinese_Taiwan," and "Chinese_Hongkong" into "Chinese"
    data['language'] = data['language'].replace(["Chinese_China", "Chinese_Taiwan", "Chinese_Hongkong"], "Chinese")

    # Ensure text of sent_res are the same
    data['sent_res'] = data['sent_res'].replace(["POSITIVE"], "positive")
    data['sent_res'] = data['sent_res'].replace(["NEGATIVE"], "negative")

    # Filter data based on language
    filtered_data = data[data['language'].str.lower().isin([language.lower()])]

    # Filter data by store name
    filtered_data = filtered_data[filtered_data['title'].isin([store])]
    
    # Filter data by date range
    filtered_data = filtered_data[
        (filtered_data['date'] >= start_date) &
        (filtered_data['date'] <= end_date)
    ]
    
    return filtered_data

# Slidebar filter
st.sidebar.header("Choose your filter")
with st.sidebar.form(key ='Form Filter'):
    # user_word = st.text_input("Enter a keyword", "habs")    
    # select_language = st.radio('Tweet language', ('All', 'English', 'French'))
    # include_retweets = st.checkbox('Include retweets in data')
    # num_of_tweets = st.number_input('Maximum number of tweets', 100)

    languages = ["English", "Indonesian", "Chinese"]

    # Filter 1 (select language)
    language = st.selectbox("Select language:", languages)

    # Filter 2 (select stores)
    store_with_most_reviews = data["title"].value_counts().idxmax()
    #store = st.selectbox("Select store:", options=data["title"].unique(), index=data["title"].unique().tolist().index(store_with_most_reviews))
    store = st.selectbox("Select store:", options=data["title"].unique())

    # Filter 3 (date range)
    min_date = min(data['date'])
    max_date = max(data['date'])

    # Calculate default values within the range
    default_start_date = max_date - relativedelta(years=2) # min_date  # Set the default to the minimum date
    default_end_date = max_date  # Set the default to the maximum date

    start_date = st.date_input("Start Date", min_value = min_date, max_value = max_date, value=default_start_date)
    end_date = st.date_input("End Date", min_value = min_date, max_value = max_date, value=default_end_date)

    submitted1 = st.form_submit_button(label = 'Submit')


# Load and filter data
with st.spinner('Loading data'):
    filtered_data = load_and_filter_data(DATA_URL, language, store, start_date, end_date)


tab1, tab2, tab3 = st.tabs(["Overview", "Topic Modelling", "About"])

with tab1:
    if st.checkbox('Show filtered data'):
        st.subheader('Raw data')
        st.write(filtered_data)

    #########################################################

    # make 3 columns for first row of dashboard
    col1, col2, col3 = st.columns([35, 30, 30])

    #########################################################

    with col1:
        total_reviews = int(filtered_data["text"].count())
        st.subheader('Number of Reviews')
        st.subheader(f"{total_reviews}")

        average_rating = round(filtered_data["stars"].mean(),1)
        star_rating = ":star:" * int(round(average_rating,0))
        st.subheader('Average Star Reviews')
        st.subheader(f"{average_rating} {star_rating}")

    with col2:
        # Star Analysis Chart
        st.subheader('Star Count')

        # Group data by star review and count occurrences
        stars_counts = filtered_data['stars'].value_counts()

        plot5 = px.bar(filtered_data, x=stars_counts.values, y=stars_counts.index, orientation='h')
        plot5.update_xaxes(title='Count')
        plot5.update_yaxes(title='Star Rating')
        st.plotly_chart(plot5, use_container_width=True)

    #with col3:
        
    #########################################################

    st.subheader('Sentiment Analysis')
    col4, col5, col6 = st.columns([45, 10, 45])

    # with col4:
    #     # Sentiment Analysis Chart
    #     # Group data by sentiment and count occurrences
    #     sentiment_counts = filtered_data['sent_res'].value_counts()

    #     # Bar chart for sentiment
    #     bar_chart = px.bar(
    #         sentiment_counts, 
    #         x=sentiment_counts.index, 
    #         y=sentiment_counts.values,
    #         labels={'x': 'sent_res', 'y': 'Count'},
    #         title=f'Sentiment Distribution for {language} Reviews')
    #     st.plotly_chart(bar_chart, use_container_width=True)

    #########################################################

    with col4:
        # Group data by sentiment and count occurrences
        sentiment_counts = filtered_data['sent_res'].value_counts()
        # Pie chart for sentiment
        pie_chart = px.pie(
            values=sentiment_counts.values,
            names=sentiment_counts.index,
            hole=0.3,
            title=f'Sentiment Distribution for {language} Reviews',
            color=sentiment_counts.index,
            # set the color of positive to blue and negative to orange
            color_discrete_map={"Positive": "#1F77B4", "Negative": "#FF7F0E"},
        )
        pie_chart.update_traces(
            textposition="inside",
            texttemplate="%{label}<br>%{value} (%{percent})",
            hovertemplate="<b>%{label}</b><br>Percentage=%{percent}<br>Count=%{value}",
        )
        pie_chart.update_layout(showlegend=False)
        st.plotly_chart(pie_chart, use_container_width=True)

    #########################################################

    col7, col8, col9 = st.columns([45, 10, 45])

    #########################################################

    # Word Cloud
    # Function to preprocess text
    def preprocess_text(text):
        # Tokenize the text
        words = nltk.word_tokenize(text)
        
        # Remove stopwords
        try:
            words = [word for word in words if word.lower() not in stopwords.words("english")]
        except:
            nltk.download('stopwords')
            words = [word for word in words if word.lower() not in stopwords.words("english")]
        
        # Remove the word "ikea", "the", "ok", "la"
        # words = [word for word in words if word.lower() != "ikea"]
        words = [word for word in words if word.lower() not in ["ikea", "the", "ok", "la"]]

        # Lemmatize words
        lemmatizer = WordNetLemmatizer()
        words = [lemmatizer.lemmatize(word) for word in words]

        # Remove single letters and apostrophes
        words = [word for word in words if len(word) > 1 and not re.match(r'^[\'\w\s]*$', word)]
        
        # Join the words back into a single string
        return " ".join(words)
        #return words

    positive_reviews = filtered_data[filtered_data['sent_res'] == 'positive']
    negative_reviews = filtered_data[filtered_data['sent_res'] == 'negative']

    # Combine positive and negative reviews text for this language
    positive_text = " ".join(positive_reviews['text'])
    negative_text = " ".join(negative_reviews['text'])

    with st.spinner('Preprocessing data for wordcloud'):
        # Preprocess the text
        try:
            preprocessed_positive_text = preprocess_text(positive_text)
        except:
            nltk.download('omw-1.4') 
            nltk.download('wordnet') 
            preprocessed_positive_text = preprocess_text(positive_text)
        preprocessed_negative_text = preprocess_text(negative_text)

    # Check if the selected language is Chinese
        if language.lower() == "chinese":
            #font_path = (r"simhei\chinese.simhei.ttf")
            font_path = "chinese.simhei.ttf"
            # the path to the Chinese font file
        else:
            font_path = None  # Use the default font for other languages

    with col7:
        with st.spinner('Plotting Wordcloud'):
            # Postive Word Cloud
            st.subheader(f'Word Cloud for Positive Reviews in {language}')
            positive_wordcloud = WordCloud(
                background_color='white',
                font_path=font_path,  # Set font path based on language
            ).generate(preprocessed_positive_text)

            # Set the Word Cloud for positive reviews as plot3
            positive_wc = plt.figure(figsize=(10, 5))
            plt.imshow(positive_wordcloud, interpolation='bilinear')
            plt.axis('off')
            st.pyplot(positive_wc)

    with col9:
        with st.spinner('Plotting Wordcloud'):
            # Negative Word Cloud
            st.subheader(f'Word Cloud for Negative Reviews in {language}')
            negative_wordcloud = WordCloud(
                background_color='white',
                font_path=font_path,  # Set font path based on language
            ).generate(preprocessed_negative_text)

            # Set the Word Cloud for negative reviews as plot3
            negative_wc = plt.figure(figsize=(10, 5))
            plt.imshow(negative_wordcloud, interpolation='bilinear')
            plt.axis('off')
            st.pyplot(negative_wc)

    #########################################################


with tab2:
    st.subheader('Topic Modelling')

    # # Using Zero-shot classification
    # # Initialize the zero-shot classification pipeline

    # @st.cache_data
    # def get_zero_shot_model():
    #     # # Check if the selected language is Chinese
    #     # if language.lower() == "chinese":
    #     #     return pipeline('zero-shot-classification', model='joeddav/xlm-roberta-large-xnli') #cant access without permission
    #     # else:
    #     #     return pipeline('zero-shot-classification', model='joeddav/distilbert-base-uncased-agnews-student')
        
    #     return pipeline('zero-shot-classification', model='joeddav/distilbert-base-uncased-agnews-student')
    #     # return pipeline('zero-shot-classification', model='joeddav/xlm-roberta-large-xnli', use_auth_token='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')

    # with st.spinner("Downloading zero shot classifier"):
    #   classifier = get_zero_shot_model()
    
    # # labels = ['retail', 'food', 'facilities']
    # labels = ['car park', 'food', 'environment','services','price','furniture']
    # # labels = ['car park', 'food', 'environment', 'retail']

    # start_modelling_time = datetime.now()
    # st.write(start_modelling_time)

    # @st.cache_data 
    # def predict_category(text):
    #     return  text.apply(lambda x: classifier(x, labels)['labels'][0])
    
    # with st.spinner('Building topic modelling'):
    #     # Apply the predict_categories function to all rows in the dataset
    #     # filtered_data['Predicted Category'] = filtered_data['text'].apply(predict_categories)
    
    #     filtered_data['Predicted Category'] = predict_category(filtered_data['text_short'])
                                                    
    #     # Display the dataset with the predicted categories
    #     selected_labels = st.multiselect("Select store:", options=labels, default = labels)

    #     st.write("Predicted Categories for Each Text:")
    #     filtered_data_class = filtered_data[filtered_data['Predicted Category'].isin(selected_labels)]
    #     st.write(filtered_data_class[['text_short', 'Predicted Category', 'sent_res']])

    #     category_counts = filtered_data['Predicted Category'].value_counts()
    #     category_counts_sorted = category_counts.sort_values(ascending=False)

    #     plot_category = px.bar(filtered_data, x=category_counts_sorted.values, y=category_counts_sorted.index, orientation='h')
    #     st.plotly_chart(plot_category, use_container_width=True)
    # end_modelling_time = datetime.now()

    # st.write(end_modelling_time - start_modelling_time)

    # def generate_word_cloud(text, title, font_path=None):
    #     st.subheader(title)
    #     wordcloud = WordCloud(
    #         background_color='white',
    #         font_path=font_path,
    #     ).generate(text)

    #     # Create a figure for the word cloud and display it
    #     wc_figure, ax = plt.subplots(figsize=(10, 5))
    #     ax.imshow(wordcloud, interpolation='bilinear')
    #     ax.axis('off')
    #     return wc_figure
    
    # # Find the top category labels if category counts is more than 100 else word cloud won't be able to display
    # # top_labels = category_counts.index[:2]
    # top_labels = category_counts[category_counts >= 100].index

    # # Generate and display word clouds for positive and negative sentiments by looping through the labels and create word clouds
    # for label in top_labels:
    #     st.subheader(f'Word Clouds for {label} in {language}')
        
    #     # Filter the data for the current label
    #     label_data = filtered_data_class[filtered_data_class['Predicted Category'] == label]

    #     # Separate positive and negative sentiments
    #     positive_text = " ".join(label_data[label_data['sent_res'] == 'positive']['text_short'])
    #     negative_text = " ".join(label_data[label_data['sent_res'] == 'negative']['text_short'])

    #     # Preprocess the text for the word clouds
    #     preprocessed_positive_text = preprocess_text(positive_text)
    #     preprocessed_negative_text = preprocess_text(negative_text)

    #     # Generate and display the word clouds for positive and negative sentiments
    #     # Create two columns for positive and negative word clouds
    #     col1, col2 = st.columns(2)

    #     with col1:
    #         positive_wc_figure = generate_word_cloud(preprocessed_positive_text, f'Positive', font_path=font_path)
    #         st.pyplot(positive_wc_figure)

    #     with col2:
    #         negative_wc_figure = generate_word_cloud(preprocessed_negative_text, f'Negative', font_path=font_path)
    #         st.pyplot(negative_wc_figure)

    #########################################################

    # #Using LDA
    # st.subheader("LDA")
    # # Preprocess the text data for topic modeling
    # filtered_tokens = filtered_data['text'].apply(preprocess_text)

    # # Initialize the CountVectorizer
    # vectorizer = CountVectorizer()

    # # Fit the vectorizer with preprocessed comments
    # token_matrix = vectorizer.fit_transform(filtered_tokens)

    # # Initialize LatentDirichletAllocation model
    # num_topics = 10  # specify the number of topics
    # lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)

    # # Fit the model with the token matrix
    # lda_model.fit(token_matrix)

    # # Print the top words for each topic
    # feature_names = vectorizer.get_feature_names_out()
    # for topic_idx, topic in enumerate(lda_model.components_):
    #     top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]  # only select 10 top words of the topic
    #     st.write(f"Topic {topic_idx + 1}: {' '.join(top_words)}")

#########################################################

with tab3:
    st.header("About")

    st.write("This dashboard displays analysis from IKEA Malaysia reviews obtained from Google.")
    st.write(f"Date range of data ranges from {min_date} to {max_date}.")

    st.write("You may select the filter for analysis to be display. Do click the Submit button for the analysis to run.")